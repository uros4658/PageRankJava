A Unified Framework for Calculating PageRank
In this report, we introduce a comprehensive framework for calculating the PageRank of nodes in a graph. The key idea is to separate the job-producing component (Queue) from the job-consuming component (Processor). Using this abstraction, we demonstrate how to solve the problem serially, with multiple threads, with message passing between multiple single-threaded processes (MPI), and finally with MPI and multi-threading.

1 Introduction
PageRank is an algorithm used by Google Search to rank web pages in their search engine results. It is a way of measuring the importance of website pages. The PageRank algorithm outputs a probability distribution used to represent the likelihood that a person randomly clicking on links will arrive at any particular page.

The PageRank of a page is defined recursively and depends on the number and PageRank metric of all pages that link to it (“incoming links”). A page that is linked to by many pages with high PageRank receives a high rank itself.

2 Design
In order to parallelize our computation, we must decide on a way to distribute the work among several (abstract) workers. Assuming that we have a fixed number of workers throughout the program execution, it would be the easiest to assign exactly one region of the resulting image to each worker. This choice is reasonable, since then each worker would only need to store the data in the region it is responsible for, and nothing else.

However, one drawback of this approach is that, due to the structure of the PageRank algorithm, some areas of the set (particularly those inside and at the border of the set) require a much greater amount of processing than the others. Whatever fixed division of the image we impose, we risk having some workers (and thus processing power in general) unused while they are waiting for the longer-running workers to complete.

This design decision implies that we should design our program around a job queue, and possibly multiple workers that fetch jobs from that queue. At the end, the escape-times are submitted back to the “owner” of the queue, which will reconstruct the wanted image from the blocks.

3 Problem description
Before describing the Queue and Processor interfaces and their implementations, we will first describe the problems these interfaces are designed to solve. To do that, we will specify how exactly should each of the 4 execution modes of the program behave.

Serial mode: This is the most basic mode, that uses a single thread for everything. First should load the job specification (the resolution of the image being rendered, and its position and dimensions in the Mandelbrot-space). Then, it should split the job into a number of smaller jobs (for compatibility with other modes), and place them into a queue. Finally, it should process each job, and render the resulting image.
Parallel (multi-threaded) mode: Here, the job loading and splitting part is the same as in the serial mode. However, once the job queue is filled, it should spawn some worker threads and coordinate job division between them. After the worker threads have finished their work, they should send back their chunks to the main thread, which will reconstruct the final image.
Distributed (MPI) mode: In this mode, we have to have at least two processes running, the server and at least one worker. The server handles the job loading and splitting part, and waits for job requests from the workers. The workers should in turn request jobs from the server, and process them. When there are no more jobs left to process, the workers should send back their chunks to the server, which will use them to reconstruct the final image.
4 The Processor and Queue interfaces
From the previous section we can see that each execution mode has roughly two independent parts:

The Queue, responsible for containing the actual job queue, as well as the supporting operations of loading the job specification, generating the jobs, acting as a sink for the processed jobs (and reassembling them later), and
The Processor, responsible for fetching jobs from the queue, processing them, and submitting the results back to the Queue.
These parts are implemented as abstract classes with the following interfaces. In order to add a new Queue or Processor, one just has to implement the unimplemented functions in these interfaces. In our case, we can get away with 2 types of Queues and 3 types of Processors, that get dispatched at runtime, based on the command line arguments given to the program. Their details follow.

5 Technical remarks
The program was implemented in Java, using a few helper libraries. As an academic exercise, some parts of the program depend on features from the draft Java 8 standard, however they are mostly syntactical sugar, by their nature. Moreover, the program is designed to be as portable as possible, and can be run on both Windows and Linux, provided that they have a recent-enough Java compiler.

Most of the program configuration (including the job specification) is loaded from a JSON file, and the output is written to a Portable PixMap (PPM) image, since it is the simplest image format which is supported by mainstream image viewers.

A technically interesting part of the implementation was implementing a modern object-oriented and templated wrapper for the C-language MPI bindings. Using template metaprogramming, one is able to send and receive arbitrary datatypes using generic Send and Recv functions. This eliminates the tedium of manually explaining the data structure layout to MPI.

6 Results
The program was evaluated on a 4-node cluster, where each node has two 4-core Intel Xeon E5345 processors. The parameters that were varied were the number of nodes (1 to 4), number of processes per node (1 to 8), and the number of threads per process (1 to 8). For each triple of parameters, the measurement was performed 5 times, and the average of those measurements is taken as the result.

7 Conclusion
As can be seen from the results, the presented algorithm works the best when there is only 1 process per node, that uses all hardware threads that are available, by itself. Since the problem is computationally-bound, we see that there is no benefit of using more cores then there are available on a node, and we can see from figure 4 that core oversubscription can result in up to 50% performance degradation. We can also see that we get diminishing returns from increasing the number of nodes. This is most likely due to Amdahl’s law, and can be remedied by reducing the communication overhead between nodes – e.g. by increasing the MPI Virtual Queue buffer size.

To sum up, the presented method is appropriate for calculating PageRank at very high resolutions. Using the best practices from software engineering, a simple framework was developed, upon which robust serial, parallel and distributed backends were built.




